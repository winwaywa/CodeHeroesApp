# app/chat_conversasion.py
from __future__ import annotations
import json
from typing import Dict, List, Tuple, Any, Optional

from chat.chat_message import ChatMessage
from chat.llm.chat_client import ChatClient
from chat.tools import TOOLS
from config.logging import logger
from retriever.pinecone.rule.rule_retriever import PineconeRuleRetriever
from stores.session_state_store import SessionState, SessionStateStore
from utils.markdown import extract_code_block

from chat.prompts import build_rule_answer_prompt
from chat.prompts import build_fix_prompt, build_summary_prompt, build_system_context
from utils.tokens import count_tokens_tiktoken

def _safe_json_parse(s: Optional[str]) -> Dict[str, Any]:
    if not s:
        return {}
    try:
        return json.loads(s)
    except Exception as e:
        logger.warning(f"[chat] L·ªói parse tool args: {e}")
        return {}

def _format_chat_messages(msgs: List[ChatMessage]) -> str:
    """
    Convert list ChatMessage th√†nh string d·ªÖ ƒë·ªçc ƒë·ªÉ log.
    M·ªói message s·∫Ω hi·ªÉn th·ªã ·ªü d·∫°ng:
    [role] n·ªôi dung
    """
    lines = []
    for i, msg in enumerate(msgs, start=1):
        role = msg.role
        content = msg.content.strip() if msg.content else ""
        lines.append(f"{i:02d}. [{role}] {content}")
    return "\n".join(lines)

def _build_messages_with_budget(
    *,
    base_messages: ChatMessage,
    chat_history: List[Dict[str, str]],
    new_user_text: str,
    model: str,
    max_turns: int = 10,
    max_tokens: int = 8000,
) -> List[ChatMessage]:
    """
    L·∫•y t·ªëi ƒëa max_turns l∆∞·ª£t chat g·∫ßn nh·∫•t + base_messages + user request m·ªõi nh·∫•t.
    N·∫øu t·ªïng token > max_tokens, ta gi·∫£m d·∫ßn s·ªë l∆∞·ª£t cho ƒë·∫øn khi ph√π h·ª£p.
    """
    # Chu·∫©n h√≥a l·ªãch s·ª≠ chat th√†nh ChatMessage list
    history_msgs = [ChatMessage(m["role"], m["content"]) for m in chat_history]

    # Tin nh·∫Øn ng∆∞·ªùi d√πng m·ªõi
    new_user_msg = ChatMessage("user", new_user_text)

    # Th·ª≠ l·∫•y t·ª´ 10 ‚Üí 0 l∆∞·ª£t g·∫ßn nh·∫•t
    for keep in range(max_turns, -1, -1):
        trial_messages = [base_messages] + history_msgs[-keep:] + [new_user_msg]
        token_count = count_tokens_tiktoken(trial_messages, model)
        logger.info(f"[chat] Th·ª≠ build messages v·ªõi {keep} l∆∞·ª£t g·∫ßn nh·∫•t: {token_count} tokens")
        if token_count <= max_tokens:
            return trial_messages  # c√πng model, ƒë·ªß token ‚Üí d√πng ngay

    # Qu√° gi·ªõi h·∫°n: ch·ªâ d√πng base + user
    return [base_messages] + [new_user_msg]

class ChatConversation:
    def __init__(self, *, client: ChatClient, state_store: SessionStateStore):
        self.client = client
        self.state_store = state_store
        self.rule_retriever = PineconeRuleRetriever(
            index_name="code-rules"
        )

    def _summarize_changes(
        self, *, model: str, language: str, base_code: str, fixed_code: str
    ) -> str:
        """ G·ªçi LLM ƒë·ªÉ t√≥m t·∫Øt thay ƒë·ªïi gi·ªØa base_code v√† fixed_code. Tr·∫£ v·ªÅ chu·ªói t√≥m t·∫Øt. """
        logger.info("[chat] G·ªçi LLM ƒë·ªÉ t√≥m t·∫Øt thay ƒë·ªïi code")

        prompt = build_summary_prompt(language=language, base_code=base_code, fixed_code=fixed_code)
        messages = [ChatMessage("system", prompt["system"]), ChatMessage("user", prompt["user"])]
        logger.info(f"[chat] Messages llm t√≥m t·∫Øt thay ƒë·ªïi: \n{_format_chat_messages(messages)}")

        try:
            return (self.client.chat_completion(
                model=model,
                messages=[ChatMessage("system", prompt["system"]), ChatMessage("user", prompt["user"])],
                temperature=0.1,
            ) or "").strip()
        except Exception as e:
            logger.exception(f"[chat] L·ªói LLM khi t√≥m t·∫Øt thay ƒë·ªïi code: {e}")
            return ""

    def _handle_fix_code(
        self, *, model: str, language: str, base_code: str, fix_instructions: str
    ) -> Tuple[Optional[str], str]:
        """ Th·ª±c hi·ªán fix code hi·ªán t·∫°i theo h∆∞·ªõng d·∫´n, tr·∫£ v·ªÅ (fixed_code, reply_message) """
        logger.info("[chat] G·ªçi LLM ƒë·ªÉ fix code")

        if not (base_code or "").strip():
            return None, "‚ö†Ô∏è Ch∆∞a c√≥ code ƒë·ªÉ s·ª≠a. H√£y d√°n code ho·∫∑c y√™u c·∫ßu review tr∆∞·ªõc."

        prompt = build_fix_prompt(language=language, base_code=base_code.strip(), fix_instructions=fix_instructions.strip())
        messages = [ChatMessage("system", prompt["system"]), ChatMessage("user", prompt["user"])]
        logger.info(f"[chat] Messages llm fix code: \n{_format_chat_messages(messages)}")

        try:
            fixed_md = self.client.chat_completion(
                model=model,
                messages=messages,
                temperature=0.1,
            )
        except Exception as e:
            logger.exception(f"[chat] L·ªói LLM khi th·ª±c hi·ªán fix code: {e}")
            return None, "Kh√¥ng th·ªÉ k·∫øt n·ªëi model ƒë·ªÉ ch·∫°y fix. Ki·ªÉm tra c·∫•u h√¨nh Provider/API key."

        # Extract code ƒë√£ fix
        fixed_code = extract_code_block(fixed_md)
        if not fixed_code:
            return None, ("‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c b·∫£n s·ª≠a. H√£y m√¥ t·∫£ r√µ h∆°n y√™u c·∫ßu fix "
                          "(v√≠ d·ª•: 'theo PEP8, th√™m type hints, gi·ªØ nguy√™n logic').")
        
        # T√≥m t·∫Øt thay ƒë·ªïi
        summary = self._summarize_changes(model=model, language=language, base_code=base_code, fixed_code=fixed_code)
        if summary:
            reply = "‚úÖ T√¥i ƒë√£ th·ª±c hi·ªán ch·ªânh s·ª≠a:\n" + "\n".join(f"- {line.lstrip('- ').strip()}" for line in summary.splitlines() if line.strip())
        else:
            reply = "‚úÖ ƒê√£ √°p d·ª•ng y√™u c·∫ßu ch·ªânh s·ª≠a v√† c·∫≠p nh·∫≠t b·∫£n s·ª≠a trong panel."

        return fixed_code, reply
    
    def _answer_with_rules(
        self, *, model: str, question: str, rule_snippets: list[dict]
    ) -> str:
        """G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n RULES + QUESTION. Tr·∫£ v·ªÅ chu·ªói tr·∫£ l·ªùi."""
        logger.info("[chat] üß† G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n RULES (context-grounded)")

        prompt = build_rule_answer_prompt(question=question, rule_snippets=rule_snippets)
        messages = [
            ChatMessage("system", prompt["system"]),
            ChatMessage("user", prompt["user"]),
        ]
        logger.info(f"[chat] Messages LLM (answer-with-rules):\n{_format_chat_messages(messages)}")

        try:
            reply = self.client.chat_completion(
                model=model,
                messages=messages,
                temperature=0.1,
            ) or ""
            return reply.strip()
        except Exception as e:
            logger.exception(f"[chat] ‚ùå L·ªói LLM khi tr·∫£ l·ªùi d·ª±a tr√™n RULES: {e}")
            return "Hi·ªán m√¨nh kh√¥ng th·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu. B·∫°n c√≥ mu·ªën m√¨nh s·ª≠a code lu√¥n kh√¥ng?"

    def _handle_search_rule(self, *, args: dict, language: str, question: str, model: str) -> str:
        query = (args.get("query") or question or "").strip()
        lang = (args.get("language") or language or "").strip()
        if not query or not lang:
            return "Thi·∫øu t·ª´ kh√≥a ho·∫∑c ng√¥n ng·ªØ ƒë·ªÉ t√¨m rule."

        # 1) G·ªçi retriever
        res = self.rule_retriever.search(query=query, language=lang, k=6, score_threshold=0.25)

        if res.hits == 0:
            return "Kh√¥ng t√¨m th·∫•y rule ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n !"

        # 2) T√≥m t·∫Øt b·∫±ng LLM
        return self._answer_with_rules(
            model=model,
            question=question,
            rule_snippets=[s.__dict__ for s in res.snippets],
        )
        
    def _call_llm_with_tools(
        self,
        *, model: str, base_messages: ChatMessage, chat_history: List[Dict[str, str]], question: str
    ) -> Dict[str, Any]:
        """ G·ªçi LLM v·ªõi tool h·ªó tr·ª£, tr·∫£ v·ªÅ raw response t·ª´ LLM. """
        logger.info("[chat] G·ªçi LLM v·ªõi tool h·ªó tr·ª£")

        messages = _build_messages_with_budget(
            base_messages=base_messages,        # list[ChatMessage] (system + context)
            chat_history=chat_history,      # list[dict] [{role, content}]
            new_user_text=question,
            model=model,                    # t√™n model ƒëang d√πng
            max_turns=10,                   # t·ªëi ƒëa 10 l∆∞·ª£t g·∫ßn nh·∫•t
            max_tokens=8000,                # gi·ªõi h·∫°n model (8k, 16k, 128k...)
        )

        logger.info(f"[chat] Messages llm c√≥ tool: \n{_format_chat_messages(messages)}")

        try:
            raw = self.client.chat_completion(
                model=model,
                messages=messages,
                tools=TOOLS,
                tool_choice="auto",
                return_raw=True,
            )
        except Exception as e:
            logger.exception(f"[chat] L·ªói g·ªçi LLM chatbot: {e}")
            raise
        return raw or {}

    # --- API ch√≠nh ---
    def reply(self, *, question: str) -> Tuple[str, SessionState, bool]:
        state = self.state_store.get()

        model = state.model
        chat_history = state.chat_messages or []
        origin_code = state.origin_code or ""
        language = state.language or "text"
        latest_fixed = (state.fixed_code or "").strip()

        # System context + system chat
        system_context = build_system_context(origin_code=origin_code, latest_fixed=latest_fixed, language=language)
        base_msgs = ChatMessage("system", system_context)

        # G·ªçi LLM v·ªõi tool h·ªó tr·ª£
        try:
            raw = self._call_llm_with_tools(
                model=model, base_messages=base_msgs, chat_history=chat_history, question=question
            )
        except Exception:
            logger.info("[chat] Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c model")
            return ("Kh√¥ng th·ªÉ k·∫øt n·ªëi model. Ki·ªÉm tra c·∫•u h√¨nh Provider/API key.", state, False)

        choices = raw.get("choices") or []
        if not choices:
            logger.info("[chat] LLM kh√¥ng tr·∫£ v·ªÅ l·ª±a ch·ªçn")
            return ("M√¨nh ch∆∞a nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ model. B·∫°n th·ª≠ h·ªèi l·∫°i nh√©.", state, False)

        message: Dict[str, Any] = (choices[0].get("message") or {})
        content: str = (message.get("content") or "").strip()
        tool_calls = message.get("tool_calls") or []

        if tool_calls:
            tc = tool_calls[0] or {}
            fn = (tc.get("function") or {})
            name = (fn.get("name") or "").strip()
            args_raw = fn.get("arguments")
            args = _safe_json_parse(args_raw) if not isinstance(args_raw, dict) else (args_raw or {})

            if name == "search_rule":
                reply = self._handle_search_rule(args=args, language=language, question=question, model=model)
                return (reply, state, False)

            if name == "run_fix":
                base_code = (latest_fixed or origin_code or "").strip()
                if not base_code:
                    return None, "‚ö†Ô∏è Ch∆∞a c√≥ code ƒë·ªÉ s·ª≠a. H√£y d√°n code ho·∫∑c y√™u c·∫ßu review tr∆∞·ªõc."
                raw_ins = args.get("fix_instructions", question)
                if isinstance(raw_ins, (list, tuple)):
                    raw_ins = "\n".join(map(str, raw_ins))
                elif not isinstance(raw_ins, str):
                    raw_ins = str(raw_ins or "")
                fix_instructions = raw_ins.strip()

                fixed_code, reply_msg = self._handle_fix_code(
                    model=model, language=language, base_code=base_code, fix_instructions=fix_instructions
                )
                # C·∫≠p nh·∫≠t code ƒë√£ fix v√†o state
                if fixed_code:
                    state.fixed_code = fixed_code

                return (reply_msg, state, True)

        # Kh√¥ng c√≥ tool-call -> tr·∫£ l·ªùi tr·ª±c ti·∫øp
        if not content:
            content = "B·∫°n mu·ªën m√¨nh gi·∫£i th√≠ch/ƒë√°nh gi√° ph·∫ßn n√†o c·ªßa code?"
        return (content, state, False)
